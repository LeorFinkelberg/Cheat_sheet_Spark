\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
		]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Полезные конструкции Spark\\в реализациях на Scala и Python}

\author{}

\date{}
\maketitle

\thispagestyle{fancy}

\tableofcontents


\section{Полезные ресурсы по Spark}

Очень крутая книга \href{https://the-internals-of-spark-sql.readthedocs.io/connector/SupportsRead/}{\textit{The Internals of Spark SQL}} по внутреннему устройству Spark от Jacek Laskowski. GitHub-репозиторий \href{https://github.com/jaceklaskowski/mastering-spark-sql-book}{книги}.

Официальная документация по ML \url{https://spark.apache.org/docs/1.2.2/ml-guide.html}.

Официальная документация по MLlib \url{https://spark.apache.org/docs/latest/ml-guide.html}.

Официальная документация по \texttt{sbt} \url{https://www.scala-sbt.org/1.x/docs/sbt-by-example.html}.


\section{Общие сведения}

\texttt{Apache Spark} -- это универсальная и высокопроизводительная кластерная вычислительная платформа \cite{carey:spark-2015}. Благодаря разнопрофильным инструментам для аналитической обработки данных, \texttt{Apache Spark} активно используется в системах интернета вещей на стороне IoT-платформы, а также в различных бизнес-приложениях, в т.ч. на базе методов машинного обучения.

\texttt{Apache Spark} позиционируется как средство потоковой обработки больших данных в реальном времени. Однако, это не совсем так: в отличие, например, от \texttt{Apache Kafka} или \texttt{Apache Storm}, фреймворк \texttt{Apache Spark} разбивает непрерывный поток данных на набор \emph{микро-пакетов}. Поэтому возможны некотрые временные задержки порядка секунды. Официальная документация утверждает, что это не оказывает большого влияния на приложения, поскольку в большинстве случаев аналитика больших данных выполняется не непрерывно, а с довольно большим шагом около пары минут. 

Однако, если все же временная задержка обработки данных (latency) -- это критичный момент для приложения, то \texttt{Apache Spark Streaming} не подойдет и стоит рассмотреть альтернативу в виде \texttt{Apache Kafka Streams}\footnote{Apache Kafka Streams -- это клиентская библиотека для разработки \emph{распределенных потоковых приложений} и \emph{микросервисов}, в которых входные и выходные данные хранятся в кластерах Kafka. Поддерживает только Java и Scala} (задержка не более 1 миллисекунды) или \emph{фреймворков потоковой обработки больших данных} \texttt{Apache Storm}, \texttt{Apache Flink} и \href{https://samza.apache.org/}{\ttfamily Apache Samza}.

В отличие от классического MapReduce\footnote{Модель распределенных вычислений}, реализованном в \texttt{Apache Hadoop}, \texttt{Spark} не записывает промежуточные данные на диск, а размещает их в оперативной памяти. Поэтому сервера, на которых развернут \texttt{Spark}, требуют большого объема оперативной памяти. Это в свою очередь ведет к удорожанию кластера. 

\texttt{Spark} вращается вокруг концепции \emph{устойчивого распределенного набора данных} (Resilient Distributed Dataset, {RDD}) \url{https://spark.apache.org/docs/latest/rdd-programming-guide.html}, который представляет собой отказоустойчивый набор элементов, с которыми можно работать \emph{параллельно}.

Существует два способа создать RDD:

\begin{itemize}
	\item распараллеливание существующего набора данных,
	
	\item на основе набора данных внешней системы хранения, такой как общая файловая система, HDFS, HBase или на основании любого другого источника, который поддерживает Hadoop.
\end{itemize}

Модуль \href{https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read%20csv}{\texttt{pyspark.sql.SparkSession}} является базовой <<точкой входа>> для работы с \texttt{DataFrame} и \texttt{SQL}. Класс \texttt{SparkSession} может использоваться для работы с объектом \texttt{DataFrame}, регистрации его как таблицы, выполнения \texttt{SQL}-запросов, кеширования таблиц и чтения \texttt{parquet}-файлов:

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: from pyspark.conf import SparkConf
In[]: from pyspark.sql import SparkSession

In[]: spark = (
          SparkSession.
          builder.  # создать экземпляр класса SparkSession
          master('local[4]').   # задает URL-адрес
                                # в данном случае подключается локально и использует 4 ядра
          appName('test app').  # задать наименование приложения
          config(conf=SparkConf()).  # задать конфигурацию
          getOrCreate()  # возвращает существующий сеанс Spark или, если его нет, создает
                         # новый сеанс на основе параметров, заданных в builder
      )
\end{lstlisting}

\section{Управление зависимостями проекта с помощью \texttt{build.sbt}}

При работе со Scala-проектом с помощью \texttt{sbt} или IntelliJ IDEA версия языка определяется параметром \texttt{scalaVersion} в файле сборки \texttt{build.sbt}, например
\begin{lstlisting}[
	style = scala,
	numbers = none	
	]
	scalaVersion := "2.12.12"
	...
\end{lstlisting}

Остается только при запуске сессии в REPL набрать \texttt{sbt console} (а не \texttt{scala}), чтобы загрузить \emph{указанную} версию Scala и все зависимости проекта.

В файл сборки \texttt{build.sbt} следует добавить следующие строки
\begin{lstlisting}[
	title = {\sffamily Пример файла build.sbt},
	style = scala,
	numbers = none	
	]
	name := "SparkML"
	
	version := "1.0"
	
	scalaVersion := "2.12.12"
	
	libraryDependencies ++= Seq(
	"org.apache.spark" %% "spark-sql" % "3.0.1" % "provided",
	"org.apache.spark" % "spark-mllib_2.12" % "3.0.1" % "provided"  // в строке используется один "%"!!!
	)
\end{lstlisting}

Для того чтобы \texttt{sbt} работал корректно, требуется разместить \texttt{AppFileName.scala} и \texttt{build.sbt} следующим образом:
\begin{itemize}
	\item файл \texttt{build.sbt} должен лежать в корне проекта,
	
	\item а scala-скрипт -- по пути \directory{src > main > scala > AppFileName.scala}.
\end{itemize}

Scala-сценарий может лежать и <<глубже>>, например, в \directory{src > main > scala > com > clairvoyant > insight > bigdata}, но тогда в scala-сценарии нужно будет нестандартный путь описать как пакет
\begin{lstlisting}[
title = {\sffamily сценарий.scala},
style = scala,
numbers = none	
]
package com.clairvoyant.insight.bigdata
...
\end{lstlisting}

Теперь можно упаковать приложение
\begin{lstlisting}[
style = bash,
numbers = none	
]
sbt package
\end{lstlisting}

В поддиректории \directory{project} проекта будет файл с версией \texttt{sbt}
\begin{lstlisting}[
title = {\sffamily project/build.properties},
style = bash,
numbers = none	
]
sbt.version = 1.3.13
\end{lstlisting}

Там же можно расположить файл с описанием плагинов для \texttt{sbt}
\begin{lstlisting}[
title = {\sffamily project/plugins.sbt},
style = bash,
numbers = none	
]
addSbtPlugin("org.scalameta" % "sbt-scalafmt" % "2.4.0")
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10")
\end{lstlisting}

В корне проекта можно расположить конфигурационный файл для \texttt{scalafmt}
\begin{lstlisting}[
title = {\sffamily .scalafmt},
style = bash,
numbers = none	
]
version = "2.6.4"

align.preset = more  // For pretty aligment
maxColumn = 100 // For my wide 30" display
\end{lstlisting}

Для запуска scala-приложения используется \texttt{spark-submit}
\begin{lstlisting}[
	style = bash,
	numbers = none	
	]
	spark-submit \
	--class "AppFileName" \
	--master local \
	target/scala-2.12/app-file-name_2.12-1.0.jar
\end{lstlisting}

\section{Пример build.sbt для работы со Spark и Breeze}

Файл сборки \texttt{build.sbt} для работы со Spark и внешними библиотеками
\begin{lstlisting}[
title = {\sffamily build.sbt},
style = scala,
numbers = none	
]
name := "Custom estimator"

scalaVersion := "2.12.12"

libraryDependencies ++= {
  val sparkVer = "3.0.1"
  Seq(
    "org.scalanlp" %% "breeze" % "1.1",
    "org.scalanlp" %% "breeze-natives" % "1.1",
    "org.scalanlp" %% "breeze-viz" % "1.1",
    "org.apache.spark" %% "spark-core" % sparkVer withSources(),
    "org.apache.spark" %% "spark-mllib" % sparkVer withSources(),
    "org.apache.spark" %% "spark-sql" % sparkVer withSources(),
    "org.apache.spark" %% "spark-streaming" % sparkVer withSources(),
    "com.esotericsoftware" % "kryo" % "4.0.1"
  )
}

libraryDependencies ++= Seq(
    "org.scalatest" %% "scalatest" % "3.0.6" % "test",
    "ml.dmlc" %% "xgboost4j" % "1.3.1",
    "ml.dmlc" %% "xgboost4j-spark" % "1.3.1"
)
\end{lstlisting}



\section{Начало работы со Spark}

Отправной точкой является \texttt{SparkSession} -- создание распределенной системы для исполнения будущих вычислений
\begin{lstlisting}[
style = scala,
numbers = none	
]
import org.apache.spark.sql.SparkSession
import spark.implicits._  // важный импорт; здесь много синтаксического сахара
val spark = SparkSession.builder()
    .appName("Example app")
    .master("local[*]")
    .getOrCreate()
\end{lstlisting}

Примеры использования Spark в ML можно найти здесь \url{https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/ml}

Метод \texttt{.master(...)} (или \texttt{.setMaster(...)} в конфигурации \texttt{SparkContext}) указывает, где нужно выполнить вычисления. Например,
\begin{lstlisting}[
style = scala,
numbers = none
]
.master("yarn")  // выполнение на кластере Hadoop
.master("local")  // выполнение локально на машине
\end{lstlisting}

У \texttt{Spark} есть 3 разных API:
\begin{itemize}
	\item RDD API,
	
	\item DataFrame API (он же SQL API): не типизирован,
	
	\item DataSet API (только для \texttt{Scala}! В \texttt{Python} это не имеет смысла): Scala-вский DataSet по сути представляет собой коллекцию экземпляров строк определенного типа (то есть это типизированный DataFrame); и поэтому, когда мы применяемнапример, метод \texttt{filter}, то он применяется к каждой строке.
\end{itemize}

Различаются они в основном тем, в каком виде представлены \emph{распределенные коллекции} при вычислениях. На низком уровне все эти формы представления коллекций являются RDD.

Работу со \texttt{Spark} можно вести и через \texttt{spark-shell} (для \texttt{Scala}) или через \texttt{pyspark} (для \texttt{Python}).

Для реальных проектов требуется создать проект определенной структуры, например, так
\begin{lstlisting}[
style = scala,
numbers = none	
]
sbt new MrPowers/spark-sbt.g8
\end{lstlisting}
а затем импортировать его в ItelliJ IDEA.

Затем нужно будет собрать проект в jar-файл, перенести этот файл на кластер и запустить \texttt{spark-submit} с полученным jar-файлом.

\texttt{SparkContext} -- это предшественник \texttt{SparkSession} и используется для работы с RDD
\begin{lstlisting}[
title = {\sffamily Scala},
style = scala,
numbers = none
]
val conf = new SparkConf().setAppName(appName)
val sc = new SparkContext(conf)
\end{lstlisting}

\begin{lstlisting}[
title = {\sffamily Python},
style = IronPython,
numbers = none	
]
conf = SparkConf().setAppName(appName)
sc = SparkContext(conf=conf)
\end{lstlisting}

Сейчас к \texttt{SparkContext} напрямую обращаться не нужно. Лучше сразу создать \texttt{SparkSession}, а затем если вдруг возникнет необходимость из-под сессии вызывать контекст.

При построении DAG есть два типа операций:
\begin{itemize}
	\item \emph{Transformations} -- описание вычислений (\texttt{map}, \texttt{filter}, \texttt{groupByKey} etc.),
	
	\item \emph{Actions} -- действия, запускающие расчеты (\texttt{reduce}, \texttt{collect}, \texttt{take} etc.).
\end{itemize}

Без \emph{действий} вычисления не запускаются! Чтобы \texttt{Spark} каждый раз не вычислял весь граф заново, можно сказать \texttt{sc.textFile("...").cache()}.

Прочитать файлы (с заголовком) с локальной файловой системы в DataFrame можно так
\begin{lstlisting}[
title = {\sffamily Scala},
style = scala,
numbers = none	
]
val df = spark.read.option("header", true).csv("file.csv")
// или так
// требуется указывать полный абсолютный путь (~ не понимает)
val df = spark.read.option("header", true).csv("/Users/leor.finkelberg/Scala_projects/citibike.csv")
df.show()
val tf = spark.read.option("header", true).text("file.txt")
tf.head
\end{lstlisting}

Аналогично на Python
\begin{lstlisting}[
title = {\sffamily Python},
style = IronPython,
numbers = none	
]
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("test").master("local[*]").getOrCreate()
df = spark.read.option("header", True).csv("/Users/leor.finkelberg/Python_projects/file.csv")
\end{lstlisting}

Для того, чтобы типы полей файлов распознавались при загрузке можно использовать опцию \texttt{inferSchema}
\begin{lstlisting}[
style = scala,
numbers = none	
]
val dataCsv = spark.read
                .option("header", "true")
                .option("inferSchema", "true") // <- NB
                .csv("filename.csv")
\end{lstlisting}

Результат будет таким
\begin{lstlisting}[
style = scala,
numbers = none	
]
dataCsv.printSchema
root
 |-- fieldname1: integer (nullable = true)
 |--  fieldname2: double (nullable = true)
 ...
\end{lstlisting}

Можно передать сразу несколько пар с помощью \texttt{options} через ассоциативный массив
\begin{lstlisting}[
style = scala,
numbers = none
]
val df = spark.read.options(Map("delimiter"->",", "header"->"true")).csv("file.csv")
\end{lstlisting}

К слову, можно считать все csv-файлы из директории просто указав путь к ней
\begin{lstlisting}[
style = scala,
numbers = none	
]
val collect_csv = spark.read.csv("folder_with_csv")
\end{lstlisting}

Аналогичным образом можно записать результат вычислений в файл
\begin{lstlisting}[
style = scala,
numbers = none	
]
df.write.option("header", true).csv("from_spark.csv")  // в текущей директории будет создана директория (!) from_spark_csv, в которой будет лежать csv-файл
// или
df.write.options(Map("header"->"true", "delimiter"->",")).csv("from_spark_again.csv")
\end{lstlisting}

Дополнительно можно управлять поведением с помощью класса \texttt{SaveMode}
\begin{lstlisting}[
style = scala,
numbers = none	
]
import org.apache.spark.sql.SaveMode

df.write.mode(SaveMode.Overwrite).csv("file.csv")
df.write.mode(SaveMode.ErrorIfExists).csv("file.csv")
...
\end{lstlisting}

В Spark лучше передавать НЕ csv-файлы (НЕ следует использовать!), а \texttt{Parquet/ORC} (наилучший вариант). Для потоковой обработки (или для случаев, когда не получается работать с колоночными данными) лучше использовать \texttt{Avro} вместо JSON.

Для того чтобы результаты вычислений, представленных в виде большого числа маленьких файлов, сохранить в виде одного относительно большого нужно провести репартиционирование
\begin{lstlisting}[
style = scala,
numbers = none	
]
// hdfs не любит мелкие файлы!
df.repartition(1).write.parquet("hdfs:///parquet-files/") // сжимаем до 1 партиции
\end{lstlisting}

Можно провести партицирование папками
\begin{lstlisting}[
style = scala,
numbers = none	
]
df.write.partitionBy("year", "month").parquet("hdfs:///parquet-files/")
\end{lstlisting}

Для запуска приложения на кластере используется \texttt{spark-submit}
\begin{lstlisting}[
style = bash,
numbers = none	
]
export HADOOP_CONF_DIR=...
./bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --executor-memory 20G \
    --num-executors 50 \
    /path/to/examples.jar 1000
\end{lstlisting}

Здесь 1000 -- это аргумент, который попадет в наше приложение.

Найти скрипт \texttt{spark-submit} можно, например, здесь \directory{HOME > Anaconda3 > Lib > site-packages > pyspark > bin}.

Основные аргументы \texttt{spark-submit}:
\begin{itemize}
	\item \verb*|--driver-cores|/\verb*|--executor-cores| -- количество ядер для каждого из элементов приложения (на контейнер!); executors выполняются в отдельных контейнерах; сколько будет контейнеров зависит от YARN,
	
	\item \verb*|--driver-memory|/\verb*|--executor-memory| -- количество памяти для каждого из элементов приложения (на контейнер!),
	
	\item \verb*|--queue| -- очередь в YARN, в которой будет выполняться приложение,
	
	\item \verb*|--num-executors| -- количество executors (может быть динамическим)
\end{itemize}

Spark-приложение упаковывается в uber-jar (жирный jar), содержащий необходимые зависимости. Его можно располагать как на локальной файловой системе, так и на HDFS.

Такой jar можно собрать командой (нужен плагин \texttt{sbt-assembly})
\begin{lstlisting}[
style = bash,
numbers = none	
]
sbt assembly
\end{lstlisting}

Если хочется тащить с собой лишние зависимости, есть три варианта:
\begin{itemize}
	\item \verb*|--jars| -- указание пути к дополнительным jar-файлам,
	
	\item \verb*|--packages| -- подключение зависимости из удаленных репозиториев (см. \url{https://spark-packages.org/}); полезно скорее для интерактивных приложений
	
	\begin{lstlisting}[
	style = bash,
	numbers = none
	]
--packages datastax:spark-cassandra-connector_2.11:2.0.7
	\end{lstlisting}

    \item \texttt{CLASSPATH} -- переменная окружения, в которой можно указать дополнительные jar-файлы.
\end{itemize}

Есть два режима деплоя приложения:
\begin{itemize}
	\item \texttt{client} -- драйвер запускается \emph{локально}, executors -- на \emph{кластере},
	
	\item \texttt{cluster} -- драйвер, как и executors, запускается на \emph{кластере}.
\end{itemize}

\section{Пример использования группировки и аггрегации на Scala}

Пример подсчета слов в файле
\begin{lstlisting}[
title = {\sffamily Не самый удачный вариант},
style = scala,
numbers = none	
]
> val lines = spark.read.text("file_name.txt")
> val linesMap = lines.flatMap(_.mkString.split(" ")).map((_, 1))
// после этой операции имена столбцов будут иметь вид "_1", "_2" и т.д.
// чтобы переименовать столбцы придется воспользоваться следующей конструкцией
> val renameCols = Map("_1" -> "Language", "_2" -> "Numbers")
> val linesMapRen = linesMap
     .select(linesMap.columns.map(c => col(c)
     .as(renameCols.getOrElse(c, c))): _*)
> val wordCounts = linesMapRen.groupBy("Language").agg(sum("Counts"))
> wordCounts.show
\end{lstlisting}

Тоже самое одним запросом с фильтрацией по числу слов
\begin{lstlisting}[
style = scala,
numbers = none	
]
linesMap
  .select(
    linesMap.columns.map(
      c => col(c).as(renameCols.getOrElse(c, c))
    ): _*  // обязательно распоковать
  )
  .groupBy("Language")
  .agg(sum("Counts"))
  .where($"sum(Counts)" === 2)
  .show
\end{lstlisting}

К слову, переименовать столбцы в объекте DataFrame можно и проще 
\begin{lstlisting}[
style = scala,
numbers = none	
]
val df = ...  // "_1", "_2", "_3"
val dfRenamed = df.toDF("newName1", "newName2", "newName3")
\end{lstlisting}


\section{Создание Spark DataFrame на основе списка}

Создание объекта \texttt{Spark} \texttt{DataFrame} на основе списка

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: packages = [
          ('Ansys', 'direct', 15550),
          ('Nastran', 'iterative', 40000),
          ('Comsole', 'direct', 45000)
      ]

In[]: spark.createDataFrame(packages, ['package_name', 'solver', 'price']).collect()
Out[]:
[Row(package_name='Ansys', solver='direct', price=15550),
 Row(package_name='Nastran', solver='iterative', price=40000),
 Row(package_name='Comsole', solver='direct', price=45000)]
\end{lstlisting}

\section{Создание Spark DataFrame на основе объекта RDD}

Создание объекта \texttt{DataFrame} на основе объекта RDD

\begin{lstlisting}[
style = ironpython,
numbers = none
]
# RDD
In[]: lst = [('Alice', 18),
             ('Jhon', 22),
             ('Alex', 48)]
In[]: sc = spark.sparkContext  # <--

In[]: rdd = sc.parallelize(lst)  # pyspark.rdd.RDD; Resilient Distributed Dataset
In[]: spark.createDataFrame(rdd).collect()

In[]: df = spark.createDataFrame(rdd, ['name', 'age'])
In[]: df.collect()   # [Row(name='Alice', age=18),
                     #  Row(name='Jhon', age=22),
                     #  Row(name='Alex', age=48)]
\end{lstlisting}

\section{Создание Spark DataFrame на основе схемы StructType()}

Создание объекта \texttt{DataFrame} на основе схемы

\begin{lstlisting}[
style = ironpython,
numbers = none
]
# schema
In[]: from pyspark.sql.types import (StringType, IntegerType,
                                     StructField, StructType)

In[]: schema = StructType([
                   StructField('name', StringType(), True),  # поле 'name'
                   StructField('age', IntegerType(), True)   # поле 'age'
               ])

In[]: df = spark.createDataFrame(rdd, schema)

In[]: df.collect()  # [Row(name='Alice', age=18),
                    #  Row(name='Jhon', age=22),
                    #  Row(name='Alex', age=48)]                               
\end{lstlisting}

\section{Создание Spark DataFrame на основе pandas}

Создание объекта \texttt{Spark} \texttt{DataFrame} на основе \texttt{pandas} \texttt{DataFrame}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: data = pd.read_csv('file.csv')
In[]: df_spark = spark.createDataFrame(data).collect()
\end{lstlisting}


Использование \texttt{SQL}-запросов с объектами \texttt{Spark} \texttt{DataFrame}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: type(df)  # pyspark.sql.dataframe.DataFrame
In[]: df.collect()
Out[]:
# [Row(url='url1', ts='2018-08-15 00:00:00', service='tw', delta=1),
#  Row(url='url1', ts='2018-08-15 00:05:00', service='tw', delta=3),
#  Row(url='url1', ts='2018-08-15 00:11:00', service='tw', delta=1),
#  ...
#  Row(url='url2', ts='2018-08-15 00:26:00', service='fb', delta=13)]

In[]: df.createOrReplaceTempView('social_delta_tab')  # создать временную таблицу
                                                      # с именем `social_delta_tab`
In[]: sql_result = spark.sql('''
                       SELECT url, service, sum(delta) AS summa
                       FROM social_delta_tab
                       GROUP BY url, service
                   ''')
In[]: sql_result.collect()  # результат SQL-запроса
Out[]:
[Row(url='url1', service='fb', summa=360),
 Row(url='url2', service='tw', summa=1200),
 Row(url='url2', service='fb', summa=38),
 Row(url='url1', service='tw', summa=59)]
\end{lstlisting}

\section{Зарегистрировать пользовательскую функцию}

Зарегистрировать пользовательскую функцию

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: power_2 = spark.udf.register('power_2', lambda x: x**2)
In[]: spark.sql("SELECT power_2(11)").collect()  # [Row(power_2(11)='121')]

In[]: from pyspark.sql.types import IntegerType
In[]: stringLength = spark.udf.register('stringLength', lambda x: len(x), IntegerType())
In[]: spark.sql("SELECT stringLength('test')").collect()  # [Row(stringLength(test)=4)]
\end{lstlisting}

\section{Фильтрация и агрегация}

Конструкция запроса \texttt{Spark} очень похожа на конструкцию \texttt{pandas}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: df_spark = spark.createDataFrame(pd.read_csv('data.csv'))
In[]: df_spark.filter(df_spark.delta >= 30).collect()
# или так
In[]: df_spark.where(df_spark.delta >= 30).collect()

In[]: (df_spark.filter(df_spark.delta >= 30).
           groupBy('url').agg({'delta' : 'sum'}).
           collect()  # возвращает все записи в формате списка строк Row()
      )
Out[]:
[Row(url='url1', sum(delta)=293),
 Row(url='url2', sum(delta)=1180)]
\end{lstlisting}


\begin{minipage}[t]{0.45\textwidth}
Пример агрегации в \texttt{PySpark} с помощью \texttt{SQL}-запроса

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: spark.sql('''
          SELECT gender,
                 usertype,
                 max(tripduration)
          FROM data
          GROUP BY gender, usertype
          ORDER BY gender
      ''').show()
Out[]:
+------+----------+-----------------+
|gender|  usertype|max(tripduration)|
+------+----------+-----------------+
|     0|  Customer|           126180|
|     0|Subscriber|              342|
|     1|Subscriber|            40339|
|     2|Subscriber|            15905|
+------+----------+-----------------+
\end{lstlisting}
\end{minipage}
\hspace*{5mm}
\begin{minipage}[t]{0.48\textwidth}

В \texttt{pandas} решение этой задачи может быть записано в виде
\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: (data.groupby(['gender', 'usertype']).
                                     agg(np.max))
Out[]:
                   tripduration
gender usertype                
0      Customer          126180
       Subscriber           342
1      Subscriber         40339
2      Subscriber         15905
\end{lstlisting}
\end{minipage}


\section{Сводная информация}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: df_spark.describe().show()
Out[]:
+-------+----+-------------------+-------+------------------+
|summary| url|                 ts|service|             delta|
+-------+----+-------------------+-------+------------------+
|  count|  30|                 30|     30|                30|
|   mean|null|               null|   null|55.233333333333334|
| stddev|null|               null|   null|140.58049193484734|
|    min|url1|2018-08-15 00:00:00|     fb|                 1|
|    max|url2|2018-08-15 00:41:00|     tw|               645|
+-------+----+-------------------+-------+------------------+

In[]: df_spark.describe(['url']).show()
Out[]:
+-------+----+
|summary| url|
+-------+----+
|  count|  30|
|   mean|null|
| stddev|null|
|    min|url1|
|    max|url2|
+-------+----+
\end{lstlisting}

\section{Оконные функции в контексте SQL и Spark DataFrame}

\texttt{Spark SQL} поддерживает три вида оконных функций (см. \tblref{tab:sql_df_api}):

\begin{itemize}
	\item ранжирующие,
	
	\item аналитические,
	
	\item агрегатные (любую агрегатную функцию\footnote{Например, \texttt{AVG}, \texttt{SUM}, \texttt{COUNT} и пр.} можно использовать в качестве оконной функции)
\end{itemize}

Чтобы использовать оконную функцию, следует указать, что функция должна использоваться как \emph{оконная} одним из следующих способов:

\begin{itemize}
	\item добавить ключевое слово \texttt{OVER} после функции поддерживаемой \texttt{SQL}, например, \texttt{AVG(revenue) OVER (...)} или
	
	\item вызвать метод \texttt{over}, например, \texttt{rank().over(...)}.
\end{itemize}

Итак, функция <<помечена>> как оконная. Теперь можно определить спецификацию окна. Спецификация окна включает три части:

\begin{itemize}
	\item спецификация секционирования (группировка строк): определяет какие строки будут входить в одну группу,
	
	\item спецификация сортировки: определяет в каком порядке будут располагаться строки в группе,
	
	\item спецификация фрейма: определяет какие стоки будут включены в фрейм для текущей строки, основываясь на их положении относительно текущей строки.
\end{itemize}

\begin{table}[h]
	\centering
	\caption{\itshape Ранжирующие и аналитические функции \texttt{PySpark}}\label{tab:sql_df_api}
	%\renewcommand{\arraystretch}{1.05}
	\begin{tabular}{cll}
		{} & контекст {SQL} & {DataFrame API} \\ \hline\hline
		Ранжирующие функции & \texttt{rank} & \texttt{rank} \\
		\rowcolor[gray]{0.96} {} & \texttt{dense\_rank} & \texttt{denseRank} \\
		{} & \texttt{percent\_rank} & \texttt{percentRank} \\
		\rowcolor[gray]{0.96} {} & \texttt{ntile} & \texttt{ntile} \\
		{} & \texttt{row\_number} & \texttt{rowNumber} \\ \hline
		\rowcolor[gray]{0.96} Аналитические функции & \texttt{cume\_dist} & \texttt{cumeDist} \\
		{} & \texttt{first\_value} & \texttt{firstValue} \\
		\rowcolor[gray]{0.96}{} & \texttt{last\_value} & \texttt{lastValue} \\
		{} & \texttt{lag} & \texttt{lag} \\
		\rowcolor[gray]{0.96} {} & \texttt{lead} & \texttt{lead} \\
	\end{tabular}
\end{table}

В контексте \texttt{SQL} ключевые слова \texttt{PARTITION BY} и \texttt{ORDER BY} используются для определения групп в \emph{спецификации секционирования} и \emph{спецификации сортировки}, соответственно

\begin{lstlisting}[
numbers = none
]
OVER (PARTITION BY ... ORDER BY ...)
\end{lstlisting}

В контексте \texttt{DataFrame API} \emph{оконную функцию} можно объявить следующим образом

\begin{lstlisting}[
style = ironpython,
numbers = none
]
from pyspark.sql.window import Window

windowSpec = Window.partitionBy(...).orderBy(...)
\end{lstlisting}

Дополнительно требуется определить:

\begin{itemize}
	\item начальную границу фрейма,
	
	\item конечную границу фрейма,
	
	\item тип фрейма.
\end{itemize}

Существует пять типов границ:

\begin{itemize}
	\item \texttt{UNBOUNDED PRECEDING}: первая строка в группе,
	
	\item \texttt{UNBOUNDED FOLLOWING}: последняя строка в группе,
	
	\item \texttt{CURRENT ROW}: текущая строка,
	
	\item \texttt{<value> PRECEDING}: ,
	
	\item \texttt{<value> FOLLOWING}.
\end{itemize}

Различают два типа фреймов:

\begin{itemize}
	\item строковый фрейм \texttt{ROWframe}: базируется на физическом смещении относительно текущей строки. Если в качестве границы используется \texttt{CURRENT ROW}, то это означает, что речь идет о текущей строке. \texttt{<value> PRECEDING} и \texttt{<value> FOLLOWING} указывают число строк до и после текущей строки, соответственно.
	
	\item диапазонный фрейм \texttt{RANGEframe}: базируется на логическом смещении относительно положения текущей строки.
\end{itemize}

\begin{lstlisting}[
title = {\ttfamily Visual representation of frame \\[-1mm] ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING},
numbers = none
]
                     poduct     | category   |revenue
                     -----------+------------+-------
                     Bendable   | Cell phone | 3000
                     Foldable   | Cell phone | 3000 <- 1 PRECEDING
Current input row -> Ultra thin | Cell phone | 6000
                     Thin       | Cell phone | 6000 <- 1 FOLLOWING
                     Very thin  | Cell phone | 6000
\end{lstlisting}

Рассмотрим работу \texttt{RANGEframe}. Рассмотрим пример. В этом примере сортировка проводится по <<revenue>>, в качестве начальной границы используется в \texttt{2000 PRECEDING}, в качестве конечной границы - \texttt{1000 FOLLOWING}.

В контексте \texttt{SQL} этот фрейм определяется как

\begin{lstlisting}[
style = ironpython,
numbers = none
]
RANGE BETWEEN 2000 PRECEDING AND 1000 FOLLOWING
\end{lstlisting}

Границы фрейма вычисляются следующим образом: \texttt{[current revenue value - 2000; current revenue value + 1000]}, т.е. границы фрейма пересчитываются в зависимости от текущего значения строки в столбце <<revenue>>

\begin{lstlisting}[
title = {\ttfamily Visual representation of frame \\[-1mm] RANGE BETWEEN 2000 PRECEDING AND 1000 FOLLOWING \\[-1mm] (ordering expression: revenue)},
numbers = none
]
# 1 step
                     poduct     | category   |revenue
                     -----------+------------+-------
Current input row -> Bendable   | Cell phone | 3000  <-- revenue range [3000-2000=1000; 3000+1000=4000]
                     Foldable   | Cell phone | 3000  <--
                     Ultra thin | Cell phone | 5000
                     Thin       | Cell phone | 6000
                     Very thin  | Cell phone | 6000
# 2 step
                     poduct     | category   |revenue
                     -----------+------------+-------
                     Bendable   | Cell phone | 3000  <-- revenue range [3000-2000=1000; 3000+1000=4000]
Current input row -> Foldable   | Cell phone | 3000  <--
                     Ultra thin | Cell phone | 5000
                     Thin       | Cell phone | 6000
                     Very thin  | Cell phone | 6000
# 3 step
                     poduct     | category   |revenue
                     -----------+------------+-------
                     Bendable   | Cell phone | 3000  <-- revenue range [5000-2000=3000; 5000+1000=6000]
                     Foldable   | Cell phone | 3000  <--
Current input row -> Ultra thin | Cell phone | 5000  <--
                     Thin       | Cell phone | 6000  <--
                     Very thin  | Cell phone | 6000  <--
# 4 step
                     poduct     | category   |revenue
                     -----------+------------+-------
                     Bendable   | Cell phone | 3000
                     Foldable   | Cell phone | 3000 
                     Ultra thin | Cell phone | 5000  <-- revenue range [6000-2000=4000; 6000+1000=7000]
Current input row -> Thin       | Cell phone | 6000  <--
                     Very thin  | Cell phone | 6000  <--
# 5 step
                     poduct     | category   |revenue
                     -----------+------------+-------
                     Bendable   | Cell phone | 3000
                     Foldable   | Cell phone | 3000 
                     Ultra thin | Cell phone | 5000  <-- revenue range [6000-2000=4000; 6000+1000=7000]
                     Thin       | Cell phone | 6000  <--
Current input row -> Very thin  | Cell phone | 6000  <--
\end{lstlisting}

Итак, чтобы определить спецификацию окна в контексте \texttt{SQL} используется конструкция

\begin{lstlisting}[
numbers = none
]
OVER (PARTITION BY ... ORDER BY ... frame_type BETWEEN start AND end)
\end{lstlisting}
где
\texttt{frame\_type} может быть либо \texttt{ROWS} (\texttt{ROWframe}), либо \texttt{RANGE} (\texttt{RANGEframe}); \texttt{start} может принимать одно из следующих значений \texttt{UNBOUNDED PRECEDING}, \texttt{CURRENT ROW}, \texttt{<value> PRECEDING} и \texttt{<value> FOLLOWING}; \texttt{end} может принимать \texttt{UNBOUNDED FOLLOWING}, \texttt{CURRENT ROW}, \texttt{<value> PRECEDING} и \texttt{<value> FOLLOWING}.

В контексте \texttt{DataFrame API} используется следующий шаблон

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: windowSpec = Window.partitionBy(...).orderBy(...)
In[]: windowSpec.rowsBetween(start, end)  # для ROW frame
In[]: windowSpec.rangeBetween(start, end)  # для RANGE frame
\end{lstlisting}

Рассмотрим другой пример

\begin{lstlisting}[
style = ironpython,
emph = {mean_udf},
numbers = none
]
In[]: from pyspark.sql.functions import pandas_udf, PandasUDFType
In[]: from pyspark.sql import Window

In[]: df = spark.createDataFrame(
               [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],
               ('id', 'v')
           )
           
In[]: @pandas_udf('double', PandasUDFType.GROUPED_AGG)
      def mean_udf(v):
          return v.mean()
# оконное преобразование        
In[]: w = Window.partitionBy('id').rowsBetween(Window.unboundedPerceding, Window.unboundedFollowing)
In[]: df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()
Out[]:
+---+----+------+
| id|   v|mean_v|
+---+----+------+
|  1| 1.0|   1.5|
|  1| 2.0|   1.5|
|  2| 3.0|   6.0|
|  2| 5.0|   6.0|
|  2|10.0|   6.0|
+---+----+------+
\end{lstlisting}

Построить кумулятивную сумму для каждой группы \texttt{PARTITION BY} (первый элемент столбца \texttt{delta} используется в качестве первого элемента нового столбца \texttt{total}, затем первый элемент столбца \texttt{delta} суммируется со вторым элементом этого же столбца, а результат записывается как второй элемент столбца \texttt{total} и т.д.)

\begin{lstlisting}[
style = ironpython,
emph = {mean_udf},
numbers = none
]
In[]: df = spark.createDataFrame(pd.read_csv('social_delta.csv'))
In[]: df.createOrReplaceTempView('social_del_tab')
In[]: spark.sql('''
         SELECT *,
             sum(delta) OVER (PARTITION BY url, service ORDER BY ts) AS total
         FROM social_del_tab 
      ''').show(3)
Out[]:
+----+-------------------+-------+-----+-----+
| url|                 ts|service|delta|total|
+----+-------------------+-------+-----+-----+
|url1|2018-08-15 00:00:00|     fb|    5|    5|  # <- 5
|url1|2018-08-15 00:05:00|     fb|   15|   20|  # <- 5 + 15 = 20
|url1|2018-08-15 00:11:00|     fb|   11|   31|  # <- 20 + 11 = 31
+----+-------------------+-------+-----+-----+
only showing top 3 rows
\end{lstlisting}

Вычислить скользящее среднее для каждой группы \texttt{PARTITION BY}

\begin{lstlisting}[
style = ironpython,
emph = {mean_udf},
numbers = none
]
In[]: df = spark.createDataFrame(pd.read_csv('social_totals.csv'))
In[]: df.createOrReplaceTempView('social_tot_tab')

In[]: df = spark.sql('''
               SELECT *,
                   AVG(total) OVER (PARTITION BY url, service ORDER BY ts
               ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS total_avg3
               FROM social_tot_tab
           ''').show(3)
Out[]:
+----+-------------------+-------+-----+------------------+
| url|                 ts|service|total|        total_avg3|
+----+-------------------+-------+-----+------------------+
|url1|2018-08-15 00:00:00|     fb|    5|               5.0|  # <- 5/1 = 5
|url1|2018-08-15 00:05:00|     fb|   20|              12.5|  # <- (5 + 20)/2 = 12.5
|url1|2018-08-15 00:11:00|     fb|   31|18.666666666666668|  # <- (5 + 20 + 31)/3 = 18.666
+----+-------------------+-------+-----+------------------+
only showing top 3 rows
\end{lstlisting}

Вычислить скользящее среднее для каждой группы, включая записи, которые отстоят от текущей записи на <<5 мин назад>>

\begin{lstlisting}[
style = ironpython,
emph = {mean_udf},
numbers = none
]
In[]: df = spark.createDataFrame(pd.read_csv('social_totals.csv', parse_dates=['ts']))
In[]: df.createOrReplaceTempView('df')

In[]: spark.sql('''
          SELECT *, AVG(total) OVER (PARTITION BY url, service ORDER BY ts
              RANGE BETWEEN INTERVAL 5 MINUTES PRECEDING AND CURRENT ROW) AS total_avg5min
          FROM df
      ''').show(3)
Out[]:
+----+-------------------+-------+-----+-------------+
| url|                 ts|service|total|total_avg5min|
+----+-------------------+-------+-----+-------------+
|url1|2018-08-15 00:00:00|     fb|    5|          5.0|  # <- 5
|url1|2018-08-15 00:05:00|     fb|   20|         12.5|  # <- (5 + 20)/2 = 12.5 (5 мин)
|url1|2018-08-15 00:11:00|     fb|   31|         31.0|  # <- 31 (6 мин)
|url1|2018-08-15 00:18:00|     fb|   45|         45.0|  # <- 45 (7 мин)
|url1|2018-08-15 00:21:00|     fb|   59|         52.0|  # <- (45 + 59)/2 = 52 (3 мин)
|url1|2018-08-15 00:30:00|     fb|   67|         67.0|
+----+-------------------+-------+-----+-------------+
only showing top 6 rows
\end{lstlisting}

Ту же задачу в \texttt{pandas} можно решить следующим образом

\begin{lstlisting}[
style = ironpython,
emph = {mean_udf},
numbers = none
]
In[]: df = pd.read_csv('social_totals.csv', parse_dates=['ts'])
In[]: df.groupby(['url', 'service']).rolling('5min', on='ts', min_periods=1).mean().reset_index(drop=True)
Out[]:
                    ts   total
0  2018-08-15 00:00:00     5.0
1  2018-08-15 00:05:00    20.0
2  2018-08-15 00:11:00    31.0
3  2018-08-15 00:18:00    45.0
4  2018-08-15 00:21:00    52.0
5  2018-08-15 00:30:00    67.0
\end{lstlisting}


\begin{minipage}[t]{0.6\textwidth}
Пусть задан объект \texttt{PySpark} \texttt{DataFrame}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: productRevenue = spark.createDataFrame([
                           ('Thin', 'Cell phone', 6000),
                           ('Normal', 'Tablet', 1500),
                           ('Mini', 'Tablet', 5500),
                           ('Ultra thin', 'Cell phone', 5000),
                           ('Very thin', 'Cell phone', 6000),
                           ('Big', 'Tablet', 2500),
                           ('Bendable', 'Cell phone', 3000),
                           ('Foldable', 'Cell phone', 3000),
                           ('Pro', 'Tablet', 4500),
                           ('Pro2', 'Tablet', 6500)],
                           ['product', 'category', 'revenue']
                       )
\end{lstlisting}
\end{minipage}
\hspace*{2mm}
\begin{minipage}[t]{0.3\textwidth}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: productRevenue.show()
Out[]:
+----------+----------+-------+
|   product|  category|revenue|
+----------+----------+-------+
|      Thin|Cell phone|   6000|
|    Normal|    Tablet|   1500|
|      Mini|    Tablet|   5500|
|Ultra thin|Cell phone|   5000|
| Very thin|Cell phone|   6000|
|       Big|    Tablet|   2500|
|  Bendable|Cell phone|   3000|
|  Foldable|Cell phone|   3000|
|       Pro|    Tablet|   4500|
|      Pro2|    Tablet|   6500|
+----------+----------+-------+
\end{lstlisting}
\end{minipage}

Требуется выявить первые два наименования наиболее дорогих продуктов из групп <<\texttt{Cell phone}>> и <<\texttt{Tablet}>>.

Решение этой задачи на основе оконных функций может выглядеть следующим образом

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: productRevenue.createOrReplaceTempView('prod_rev')
In[]: spark.sql('''
          SELECT
              product,
              category,
              revenue
          FROM (
              SELECT
                  *,
                  dense_rank() OVER (PARTITION BY category ORDER BY revenue DESC) AS rank
              FROM prod_rev)
          WHERE rank <= 2''').show()
Out[]:
+----------+----------+-------+
|   product|  category|revenue|
+----------+----------+-------+
|      Thin|Cell phone|   6000|  # <- first group
| Very thin|Cell phone|   6000|
|Ultra thin|Cell phone|   5000|
|      Pro2|    Tablet|   6500|  # <- second group
|      Mini|    Tablet|   5500|
+----------+----------+-------+
\end{lstlisting}

То есть к каждой найденной группе применяется функция \texttt{dense\_rank} с помощью \texttt{PARTITION BY} выполняется группировка по столбцу <<\texttt{category}>>. Внутри группа упорядочивается по убыванию (\texttt{ORDER BY}) по столбцу <<\texttt{revenue}>>.

Пусть теперь требуется вычислить на сколько отличается по стоимости самый дорогой продукт в группе от прочих продуктов из той же группы. Задача может быть решена так

\begin{lstlisting}[
style = ironpython,
numbers = none
]
In[]: import sys
In[]: from pyspark.sql.window import Window
In[]: import pyspark.sql.functions as func

In[]: df = productRevenue

In[]: windowSpec = (
          Window.partitionBy(df['category']).
                 orderBy(df['revenue'].desc()).
                 rangeBetween(-sys.maxsize, sys.maxsize))
                 
In[]: revenue_diff = func.max(df['revenue']).over(windowSpec) - df['revenue']

In[]: df.select(  # выбрать из объекта df соответствующие столбцы
          df['product'],
          df['category'],
          df['revenue'],
          revenue_diff.alias('revenue_diff')  # добавить в вывод этот столбец
      ).show()
Out[]:
+----------+----------+-------+------------+
|   product|  category|revenue|revenue_diff|
+----------+----------+-------+------------+
|      Thin|Cell phone|   6000|           0|  # <- первая группа
| Very thin|Cell phone|   6000|           0|
|Ultra thin|Cell phone|   5000|        1000|
|  Bendable|Cell phone|   3000|        3000|
|  Foldable|Cell phone|   3000|        3000|
|      Pro2|    Tablet|   6500|           0|  # <- вторая группа
|      Mini|    Tablet|   5500|        1000|
|       Pro|    Tablet|   4500|        2000|
|       Big|    Tablet|   2500|        4000|
|    Normal|    Tablet|   1500|        5000|
+----------+----------+-------+------------+
\end{lstlisting}

\section{Работа с файловой системой Databricks}

Databricks \url{https://databricks.com/product/unified-data-analytics-platform} -- это платформа для анализа больших данных, построенная вокруг Apache Spark. DBFS -- распределенная файловая система Databricks.

Работа с файловой системой в рамках платформы Databricks осуществляется через модуль \texttt{dbutils}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
# вывести список фалов текущей директории
dbutils.fs.ls('dbfs:/FileStore/tables')
# удалить файл из DBFS
dbuitls.fs.rm('dbfs:/FileStore/tables/file_name.csv', True)
\end{lstlisting} 

Записать Spark-объект DataFrame можно записать, к примеру, на DBFS
\begin{lstlisting}[
style = ironpython,
numbers = none
]
pandas_data = pd.DataFrame({
    'package_name' : ['Ansys', 'Nastran', 'Abaqus', 'LMS Virtual Lab', 'Comsole'],
    'solver_type' : ['direct', 'iterative', 'direct', 'iterative', 'iterative'],
    'language' : ['IronPython', 'Java', 'C++', 'Python', 'Erlang'],
    'performance' : np.abs(10*np.random.RandomState(42).randn(5))
})
data = spark.createDataFrame(pandas_data)

# сохранить объект на DBFS в формает csv
data.write.save('dbfs:/FileStore/tables/data.csv', format='csv')

# прочитать объект
spark.sql('''
    SELECT * FROM csv.`dbfs:/FileStore/tables/data.csv`
''').show()

# сохранить объект на DBFS в формате parquet
data.write.save('dbfs:/FileStore/tables/cae_packages.parquet', format='parquet')

# прочитать объект
spark.sql('''
    SELECT * FROM parquet.`dbfs:/FileStore/tables/cae_packages.parquet`
''').show()
\end{lstlisting}

Формат Parquet -- это колончный (столбцово-ориентированный) формат хранения данных, который поддерживается системой Hadoop. Он сжимает и кодирует данные, и может работать с вложенными структурами -- все это делает его очень эффективным.

К слову, удалить таблицы, находящиеся в оперативной памяти, можно так
\begin{lstlisting}[
style = ironpython,
numbers = none
]
from pyspark.sql import SQLContext

sqlcont = SQLContext(sc)

for tab in sqlcont.tableName():
    sqlcont.dropTempTable(tab)
\end{lstlisting}

\section{Приемы работы с библиотекой Breeze}

Математика в Spark, как правило, реализована с помощью Breeze. Библиотека Breeze нераспределенная!!! Предполагается, что Breeze работает над небольшими блоками данных.

Оптимизация в Breeze
\begin{lstlisting}[
style = scala,
numbers = none
]
import breeze.linalg._
import breeze.numerics._
import breeze.optimize.{DiffFunction, LBFGS}

val X = DenseMatrix.rand(2000, 3)
val y = X*DenseVector(0.5, -0.1, 0.2) // цель

val J = new DiffFunction[DenseVector[Double]] {
    def calculate(w: DenseVector[Double]) = {
        val e = X*w - y
        val loss = sum(e ^:^ 2.0) / (2 * X.rows)
        val grad = (e.t * X) /:/ (2.0 * X.rows)
        (loss, grad.t)
    }
}

val optimizer = new LBFGS[DenseVector[Double]]()
println(optimizer.minimize(J, DenseVector(0.0, 0.0, 0.0)))
// DenseVector(0.49999998855333594, -0.10000001104504522, 0.20000002605021208) // приближение к цели
\end{lstlisting}

Здесь используется метод L-BFGS. Это алгоритм оптимизации семейства квази-ньютоносвких методов, который аппроксимирует алгоритм Бройдена-Флетчера-Гольфарба-Шанно с учетом ограниченного объема компьютерной памяти. L-BFGS -- популярный алгоритм оценки параметров в машинном обучении.

\section{Spark ML Pipelines}

Различают
\begin{itemize}
	\item \emph{Transformer}: принимают на вход данные, возвращает преобразованные данные,
	
	\item \emph{Estimator}: принимает на вход данные, возвращает Transformer.
\end{itemize}

Существуют еще \emph{Model} -- это трансформер, который был получен с помощью Estimator.

Эти объекты можно собирать в конвейеры, например: \texttt{Transformer1} \textrightarrow\ \texttt{Estimator1} \textrightarrow\ \texttt{Estimator2}.

Сам по себе конвейер является Estimator, т.е. ему можно подать на вход данные. Если в конвейере есть Transformer, то данные будут преобразованы и переданы дальше. Если в конвейере будет Estimator, то данные будут поданы на вход этому Estimator, который вернет Transformer, который в свою очередь будет применен к данным и вернет преобразованные данные.

Пример
\begin{lstlisting}[
style = scala,
numbers = none	
]
import org.apache.spark.sql.SparkSession

val spark = SparkSession
              .builder.
              .appName("test")
              .master("local[*]")
              .getOrCreate()
              
val X = DenseMatrix.rand(10000, 3)
val y = X*DenseVector(0.5, -0.1, 0.2)
val data = DenseMatrix.horzcat(X, y.asDenseMatrix.t)
              
val df = spark.createDataFrame(
    data(*, ::).iterator // итератор по строкам
        .map(row => (row(0), row(1), row(2), row(3)))
        .toSeq
).toDF("x1", "x2", "x3", "y")
df.show(1)
// Вывод
+------------------+------------------+------------------+------------------+
|                x1|                x2|                x3|                 y|
+------------------+------------------+------------------+------------------+
|0.9528102359167567|0.7292676335740298|0.5690442082761085|0.5172871962561971|
+------------------+------------------+------------------+------------------+
only showing top 1 row
\end{lstlisting}

Подготовка конвейера
\begin{lstlisting}[
style = scala,
numbers = none	
]
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.regression.{
	LinearRegression, // модель ML
	LinearRegressionModel // обученная модель ML
}

val pipeline = new Pipeline().setStages(
  Array( // массив этапов
    new VectorAssembler() // трансформер
      .setInputCols(Array("x1", "x2", "x3"))
      .setOutputCol("features"), // (x1, x2, x3) -> features
    new LinearRegerssion().setLabelCol("y") // эстиматор
  )
)

val model = pipeline.fit(df)
val w = model.stages.last
  .asInstanceOf[LinearRegressionModel].coefficients
// w: org.apache.spark.ml.linalg.Vector = [0.5000000000000163,-0.09999999999999584,0.20000000000000076]

val pred = model.transform(df)
\end{lstlisting}

Здесь модель линейной регрессии принимает на вход вектор с именем \texttt{"features"} и целевой вектор \texttt{"y"}, на который мы указываем с помощью \texttt{setLabelCol("y")}.

ВАЖНО: здесь \texttt{model} это не линейная регрессия, а конвейер, в котором модель линейной регрессии лежит на последнем этапе.

Метод \texttt{transform} возвращает объект, у которого будет два новых поля (\texttt{features} и \texttt{prediction}). Поле \texttt{features} добавил \texttt{VectorAssembler}, когда собирал данные. Поле \texttt{prediction} очевидно добавила модель в качестве прогноза.

ВАЖНО: если сейчас посмотреть на схему данных \texttt{pred}, то она будет выглядеть примерно так
\begin{lstlisting}[
style = bash,
numbers = none
]
root
  |-- x1: double (nullable = false)
  |-- x2: double (nullable = false)
  |-- x3: double (nullable = false)
  |-- y: double (nullable = false)
  |-- features: vector (nullable = true) # <-- потеряли информацию о природе признаков
  |-- pred: double (nullable = false)
\end{lstlisting}

В глубоких конвейерах могут возникнуть сложности из-за потери информации о природе признаков, например, на этапе построения интерпретации. 

В Spark есть свой собственный тип векторов и матриц (это не то же самое, что векторы и матрицы Breeze)
\begin{itemize}
	\item Vector:
	\begin{itemize}
		\item DenseVector,
		
		\item SparsVector
	\end{itemize}

    \item Matrix
    \begin{itemize}
    	\item DenseMatrix,
    	
    	\item SparseMatrix (CSC)
    \end{itemize}
\end{itemize}

Есть метод \texttt{compressed}, который в зависимости от структуры вектора/матрицы принимает решение о том, в каком виде имеет смысл хранить данные (в полносвязанном или в разреженном).

Для сложных преобразований можно превратить spark-вектор/матрицу в breeze-вектор/матрицу с помощью \texttt{asBreeze}.

Извлечение атрибутов
\begin{lstlisting}[
style = scala,
numbers = none	
]
import org.apache.spark.ml.attribute.AttributeGroup

AttributeGroup.fromStructField(pred.schema("features"))
  .attributes.get.foreach(println)
// Вывод
{"type":"numeric","idx":0,"name":"x1"}
{"type":"numeric","idx":1,"name":"x2"}
{"type":"numeric","idx":2,"name":"x3"}
\end{lstlisting}

Для модульного тестирования используется ScalaTest \url{https://www.scalatest.org/user_guide}.


\section{Запросы к DataFrame с помощью методов и SQL}

Пример запроса к объекту DataFrame с использованием методов
\begin{lstlisting}[
title = {\sffamily Scala},
style = scala,
numbers = none	
]
> val df = spark.read.option("header", "true").csv("file_name.csv")
> df.show(3)
//+------------+-------+----+----------+
//|state/region|   ages|year|population|
//+------------+-------+----+----------+
//|          AL|under18|2012|   1117489|
//|          AL|  total|2012|   4817528|
//|          AL|under18|2010|   1130966|
//+------------+-------+----+----------+
//only showing top 3 rows
> df.select($"ages", $"year").filter(
  $"year" > 2010 && $"state/region" === "AL"
).orderBy($"year").show
\end{lstlisting}

Тот же самый запрос, но с использованием SQL
\begin{lstlisting}[
title = {\sffamily Scala},
style = scala,
numbers = none	
]
> df.createOrReplaceTempView("state_population")
spark.sql(
  "SELECT ages, year FROM state_population WHERE year > 2010 and `state/region` = \"AL\" ORDER BY 2;"
).show
\end{lstlisting}

В pandas этот запрос выглядел был так

\begin{lstlisting}[
title = {\sffamily Python},
style = ironpython,
numbers = none	
]
> df = pd.read_csv("file_name.csv", header=0)
> df[["ages", "year"]][
    (df["year"] > 2010) & (df["state/region"] == "AL")
].sort_values("year")
\end{lstlisting}

Или так
\begin{lstlisting}[
title = {\sffamily Python},
style = ironpython,
numbers = none	
]
df.query(
    "year > 2010 & `state/region` == 'AL'"
)[["ages", "year"]].sort_values("year")
\end{lstlisting}

Еще пример с использованием эквивалента конструкции \texttt{CASE WHEN}
\begin{lstlisting}[
title = {\sffamily Scala},
style = scala,
numbers = none	
]
df.select(// список выборки
  $"ages",
  when($"ages" === "under18", 0).
  when($"ages" === "total", 1).
  otherwise(2).as("encodedCol"),
  $"year"
).filter(
      $"year" > 2010 && $"state/region" === "AL"
    ).show
//+-------+----------+----+
//|      ages|encodedCol|year|
//+-------+----------+----+
//|under18| 0|2012|
//|total| 1|2012|
\end{lstlisting}

Или, используя SQL
\begin{lstlisting}[
title = {\sffamily Scala},
style = scala,
numbers = none	
]
spark.sql(
  "SELECT
      ages,
      CASE WHEN (ages = \"under18\") THEN 0
           WHEN (ages = \"total\") THEN 1
           ELSE 2 END AS encodedCol,
      year
    FROM state_population
    WHERE year > 2010 and `state/region` = \"AL\"
    ORDER BY 3;"
 ).show
\end{lstlisting}

\section{Случайный лес в Spark}

Пример решения задачи с использованием алгоритма случайного леса на платформе Spark
\begin{lstlisting}[
title = {\sffamily Случайный лес},
style = scala,
numbers = none	
]
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.cofiguration.Strategy
import org.apache.spark.mllib.util.MLUtils
// Загузка и парсинг данных
val data = MLUtils.loadLibSVMFile(sc, "data.txt")
// Разбиение множества данных на обучение и тест
val splits = data.randomSplit(Array(0.7, 0.3))
val (trainginData, testData) = (splits(0), splits(1))
// Обучение модели
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 200
val featureSubsetStrategy = "auto"
val model = RandomForest.trainClassifier(
    trainingData,
    treeStrategy,
    numTrees,
    featureSubsetStrategy,
    seed = 12345
)
// Проверка на тестовом наборе
val testErr = testData.map{
    point => {
    	val prediction = model.prediction(point.features)
    	if (point.label == prediction) 1.0 else 0.0
    }
}.mean()
println(testErr)
println(model.toDebugString)
\end{lstlisting}

\section{Экстремальный градиентный бустинг с XGboost4j}

Подробности в \url{https://xgboost.readthedocs.io/en/latest/jvm/}.

Тонкая настройка XGboost4j для Spark: \url{https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html}

Установить XGBoost4j для Scala можно с помощью \texttt{sbt} \url{https://xgboost.readthedocs.io/en/latest/jvm/index.html#}
\begin{lstlisting}[
title = {\sffamily build.sbt},
style = scala,
numbers = none	
]
...
libraryDependencies ++= Seq(
// здесь нужно заменить "latest_version_num" на стабильную версию, например, на "1.3.1"
"ml.dmlc" %% "xgboost4j" % "latest_version_num", 
"ml.dmlc" %% "xgboost4j-spark" % "latest_version_num"
)
\end{lstlisting}

Пример решения задачи с помощью алгоритма экстремального градиентного бустинга
\begin{lstlisting}[
style = scala,
numbers = none	
]
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}
import org.apache.spark.ml.features.StringIndexer
import org.apache.spark.ml.feature.VectorAssembler
import ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier

val spark = SparkSession.builder().getOrCreate()
val schema = new StructType(Array(
                   StructField("sepal length", DoubleType, true),
                   StructField("sepal width", DoubleType, true),
                   StructField("petal length", DoubleType, true),
                   StructField("petal width", DoubleType, true),
                   StructField("class", StringType, true)
))
val rawInput = spark.read.schema(schema).csv("input_path")

val stringIndexer = new StringIndexer().
                          setInputCol("class").
                          setOutputCol("classIndex").fit(rawInput)
val labelTransformed = stringIndexer.transform(rawInput).drop("class")
val vectorAssembler = new VectorAssembler().
                            setInputCols(Array(
                              "sepal length",
                              "sepal width",
                              "petal length",
                              "petal width"
                            )).setOutputCol("feature")
val xgbInput = vectorAssembler.transform(labelTransformer).select("features", "classIndex") // подготовленный набор (X, y)
// Тренировка
val xgbParam = Map(
                 "eta" -> 0.1f,
                 "max_depth" -> 2,
                 "objectiv" -> "multi:softprob",
                 "num_class" -> 3,
                 "num_round" -> 100,
                 "num_workers" -> 2
)
val xgbClassifier = new XGBoostClassifier(xgbParam).
                          setFeaturesCol("features").
                          setLabelCol("classIndex")
val xgbClassificationModel = xgbClassifier.fit(xgbInput)
\end{lstlisting}

Пример использования XGboost4j для решения задачи регрессии
\begin{lstlisting}[
style = scala,
numbers = none
]
import ml.dmlc.xgboost4j.scala.spark.{XGBoostRegressionModel, XGBoostRegressor} // стандартный XGBoost
import org.apache.spark.ml.evaluation.{RegressionEvaluator}
import org.apache.spark.ml.tuning.ParamGridBuilder
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{FloatType, IntegerType, StructField, StructType}
import ml.dmlc.xgboost4j.scala.spark.rapids.CrossValidator // XGBoost4j Rapids: можно использовать для повышения производительности с помощью GPU

val trainParquetPath = "/data/taxi/parquet/train"
val evalParquetPath = "/data/taxi/parquet/eval"

val labelColName = "fare_amount"
val schema = 
  StructType(Array(
    StructField("vendor_id", FloatType),
    StructField("passenger_count", FloatType),
    ...
    StructField(labelColName, FloatType),
    StructField("is_weekend", FloatType)
))

val spark = SparkSession.builder().appName("taxi-gpu-cv").getOrCreate()
val trainDs = spark.read.parquet(trainParquetPath)

val featureNames = schema.filter(_.name != lableColName).map(_.name)

val regressionParam = Map(
    "learning_rate" -> 0.5,
    "max_depth" -> 8,
    "subsample" -> 0.8,
    "gamma" -> 1,
    "num_round" -> 100,
    "tree_method" -> "gpu_hist"
)

val regressor = new XGBoostRegressor(regressorParam).
                      setLabelCol(labelColName). // целевая переменная
                      setFeaturesCols(featureNames) // признаки
val paramGrid = new ParamGridBuilder().
                      addGrid(regressor.maxDepth, Array(3, 10)).
                      addGrid(regressor.eta, Array(0.2, 0.6)).
                      build()
val evaluator = new RegressionEvaluator().setLabelCol(labelColName)
val cv = new CrossValidator(). // используется RAPIDS для параллелизации
               setEstimator(regressor).
               setEvaluator(evaluator).
               setEstimatorParamMaps(paramGrid).
               setNumFolds(3)
               
val model = cv.fit(trainDs).bestModel.asInstanceOf[XGBoostRegressionModel]
val transformDs = spark.read.parquet(evalParquetPath)
val df = model.transform(transformDs).cache() // делаем предсказания
df.select("fare_amount", "prediction").show(5)   

//val evaluator = new RegressionEvaluator().setLabelCol(labelColName)
val rmse = evaluator.evaluate(df)
spark.close()
\end{lstlisting}


\section{Spark Streaming и Kafka}

Простой пример подписки на один топик
\begin{lstlisting}[
style = scala,
numbers = none	
]
val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "quickstart-events") // <-
  .load()
  
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .writeStream
  .outputMode("append")
  .format("console")
  .start().awaitTermination()
\end{lstlisting}



\section{Распределенное глубокое обучение с Elephas}

Elephas \url{https://github.com/maxpumperla/elephas} -- это распределенная платформа глубокого обучения, построенная на связке <<Keras + Spark>>.

Пример
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
from pyspark import SparkContext, SparkConf
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD
from elephas.utils.rdd_utils import to_simple_rdd
from elephas.spark_model import SparkModel

conf = SparkConf().setAppName("Elephas_App").setMaster("local[8]")
sc = SparkContext(conf = conf)

model = Sequential()
model.add(Dense(128, input_dim=784))
model.add(Activation("relu"))
model.add(Dropout(0.2))
model.add(Dense(128))
model.add(Activation("relu"))
model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation("softmax"))
model.compile(loss="category_crossentropy", optimizer=SGD())

# RDD-подход
rdd = to_simple_rdd(sc, x_train, y_train)
spark_model = SparkModel(model, frequency="epoch")
spark_model.fit(rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1)

# DataFrame-подход. Лучше так!!!
df = to_data_frame(sc, x_train, y_train, categorical=True)
test_df = to_data_frame(sc, x_test, y_test, categorical=True)
estimator = ElephasEstimator(model, epochs=epochs, batch_size=batch_size, frequency="batch", categorical=True, nb_classes=nb_classes)
fitted_model = estimator.fit(df)
\end{lstlisting}

\section{Spark и Microsoft Machine Learning}

Очень полезная обертка для Spark \url{https://github.com/Azure/mmlspark}. 
Jupyter Notebook с примерами: \url{https://github.com/Azure/mmlspark/tree/master/notebooks/samples}

Запустить сессию на Python
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
import pyspark
spark = pyspark.sql.SparkSession.builder.appName("MyApp") \
    .config("spark.jars.packages", "com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc3") \
    .config("spark.jars.repositories", "https://mmlspark.azureedge.net/maven") \
    .getOrCreate()
import mmlspark
\end{lstlisting}


\section{Оптимизация гиперпараметров и AutoML}

Есть интересное расширение для \texttt{Spark ML} под именем \texttt{PravdaML}. Это расширение добавляет гибкости в вопросах организации потока данных, повышает коэффициент утилизации ресурсов и улучшения масштабирования ML.

\section{Apache Zookeeper}

\subsection{Общие сведения}

{Apache Zookeeper} -- это сервис распределенной координации -- централизованная служба для поддержки информации о конфигурации, обеспечения распределенной синхронизации и предоставления групповых служб. Все эти виды услуг используются в той или иной форме распределенными приложениями.

В конце сеанса следует:
\begin{itemize}
	\item Остановить продюсер, консамер с помощью \texttt{Ctrl-C},
	
	\item Остановить Kafka с помощью \texttt{Ctrl-C},
	
	\item Остановить ZooKeeper с помощью \texttt{Ctrl-C}.
\end{itemize}

Основные свойства Zookeeper:
\begin{itemize}
	\item пространство ключей образует дерево (иерархию, подобную файловой системе),
	
	\item значения могут содержаться в любом узле иерархии, а не только в листьях (как если бы файлы одновременно были бы и каталогами), узел иерархии называется znode,
	
	\item между клиентом и сервером двунаправленная связь, следовательно, клиент может подписываться как изменение конкретного значения или части иерархии,
	
	\item возможно создать временную пару ключ/значение, которая существует, пока клиент, ее создавший, подключен к серверу,
	
	\item все данные должны помещаться в память,
	
	\item устойчивость к смерти некритического количества узлов кластера.
\end{itemize}

\subsection{Установка и запуск Zookeeper}\label{subsec:zookeeper}

Чтобы установить Zookeeper на MacOS следует с официального сайта проекта скачать tar-архив \url{https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.6.2/apache-zookeeper-3.6.2-bin.tar.gz} и распаковать его, например, в поддиректорию \texttt{zookeeper} домашней директории
\begin{lstlisting}[
style = bash,
numbers = none	
]
tar -xvzf apache-zookeeper-3.6.2-bin &&\
mv apache-zookeeper-3.6.2 zookeeper-3.6.2
\end{lstlisting}

Затем нужно в конфигурационном файле командной оболочки \texttt{.bashrc}, \texttt{.zshrc} создать переменную окружения \texttt{ZOOKEEPER\_HOME}
\begin{lstlisting}[
title = {\sffamily \~{}/.zshrc},
style = bash,
numbers = none	
]
export ZOOKEEPER_HOME="/Users/leor.finkelberg/zookeeper/zookeeper-3.6.2/bin"
export PATH="$ZOOKEEPER_HOME:${PATH}"
\end{lstlisting}

Кроме того необходимо переименовать файл \texttt{zoo\_sample.cfg} в \texttt{zoo.cfg}, а затем заменить значение по умолчанию параметра \texttt{dataDir} на следующее \url{https://zookeeper.apache.org/doc/current/zookeeperStarted.html}
\begin{lstlisting}[
title = {\sffamily \~{}/zookeeper/zookeeper-3.6.2/conf},
style = bash,
numbers = none	
]
# каталог data должен существовать, иначе Zookeeper не сможет запустить сервер
dataDir=~/zookeeper/zookeeper-3.6.2/data
\end{lstlisting}
Остальные два параметра минимальной конфигурации -- \texttt{tickTime} и \texttt{clientPort} -- оставим без изменений.

Теперь можно запустить ZooKeeper
\begin{lstlisting}[
style = bash,
numbers = none	
]
zkServer.sh start 
\end{lstlisting}

Описанные выше шаги запускают ZooKeeper в автономном режиме. В этом случае не поддерживается репликация и если процесс упадет, то служба выйдет из строя. Такой схемы достаточно для большинства ситуаций, но все же, если требуется запустить ZooKeeper с поддержкой репликации, то следует ознакомиться с \url{https://zookeeper.apache.org/doc/current/zookeeperStarted.html#sc_RunningReplicatedZooKeeper}.

Далее устанавливаем соединение с ZooKeeper
\begin{lstlisting}[
style = bash,
numbers = none	
]
zkCli.sh -server 127.0.0.1:2181
\end{lstlisting}

Теперь нужно подготовить запуск Apache Kafka. Предварительно бинарные файлы можно скачать здесь \url{https://kafka.apache.org/downloads}.

Перед запуском Kafka следует указать куда будут писаться логи. Сделать это можно, изменив значение параметра \texttt{log.dirs} в файле \texttt{server.properties}
\begin{lstlisting}[
title = {\sffamily \~{}/kafka/kafka\_2.13-2.7.0/config/server.properties},
style = bash,
numbers = none
]
log.dirs=~/kafka/kafka_2.13-2.7.0/kafka-logs
\end{lstlisting}
и здесь же правим файл \texttt{zookeeper.properties}
\begin{lstlisting}[
title = {\sffamily \~{}/kafka/kafka\_2.13-2.7.0/config/zookeeper.properties},
style = bash,
numbers = none	
]
dataDir=~/kafka/kafka_2.13-2.7.0/zookeeper-data
\end{lstlisting}

А вот теперь можно запускать kafka-сессию
\begin{lstlisting}[
style = bash,
numbers = none	
]
./kafka-server-start.sh ~/kafka/kafka_2.13-2.7.0/config/server.properties
\end{lstlisting}

Для создания топика используем следующий сценарий командной оболочки
\begin{lstlisting}[
style = bash,
numbers = none	
]
./kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
# Created topic quickstart-events.
\end{lstlisting}

Посмотреть описание топика можно следующим образом
\begin{lstlisting}[
style = bash,
numbers = none	
]
./kafka-topics.sh --describe --topic quickstart-events --bootstrap-server localhost:9092
\end{lstlisting}

Клиент Kafka общается с брокерами сообщений через сеть для записи (или чтения) событий. Получив сообщение брокеры будут хранить его так долго, как это нужно. Запустим клиент продюсера, чтобы записать в топик несколько событий
\begin{lstlisting}[
style = bash,
numbers = none	
]
# запись некоторых событий в топик
./kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
>This is my first event
>This is my second events
>^C% # Ctrl-C
\end{lstlisting}

Теперь можно открыть еще один терминал и прочитать переданные в топик события
\begin{lstlisting}[
style = bash,
numbers = none
]
# чтение событий из топика
./kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
# Ctrl-C
\end{lstlisting}

Остановить ZooKeeper можно так
\begin{lstlisting}[
style = bash,
numbers = none	
]
zkServer.sh stop
\end{lstlisting}




\section{Apache Kafka}

\subsection{Установка и запуск Kafka}

ВАЖНО: перед запуском Kafka следует запустить ZooKeeper (см.~\ref{subsec:zookeeper}).

Apache Kafka -- брокер сообщений, работающий поверх сервиса Apache Zookeeper.

Простая схема: создается топик (тема), в которую будут отправляться сообщения от продюсеров, и на которую смогут подписаться консьюмеры, чтобы их получать.

\section{Apache HBase}

HBase -- распределенная нереляционная (столбцово-ориентирования) база данных формата <<ключ-значение>>. 

\subsection{Установка и запуск}

Подробности, связанные с установкой различных режимах (автономном, распределенном и т.д.) можно узнать на странице \url{https://hbase.apache.org/book.html}.

Скачать tar-архив можно здесь \url{https://www.apache.org/dyn/closer.lua/hbase/2.4.0/hbase-2.4.0-bin.tar.gz}
\begin{lstlisting}[
style = bash,
numbers = none	
]
curl -O https://apache-mirror.rbc.ru/pub/apache/hbase/2.4.0/hbase-2.4.0-bin.tar.gz
\end{lstlisting}

Теперь следует распоковать архив
\begin{lstlisting}[
style = bash,
numbers = none	
]
tar -xvzf hbase-2.4.0...
\end{lstlisting}
перейти в директорию \directory{hbase-2.4.0} и задать путь до java в файле \texttt{hbase-env.sh}, раскоментировав нужную строку
\begin{lstlisting}[
title = {\sffamily conf/hbase-env.sh},
style = bash,
numbers = none	
]
export JAVA_HOME=/usr/local/Cellar/openjdk/15.0.1
\end{lstlisting}

В конфигурационном файле команданой оболочки удобно задать переменные окружения для Java и HBase
\begin{lstlisting}[
title = {\sffamily \~{}/.zshrc},
style = bash,
numbers = none	
]
# for HBase
export JAVA_HOME="/usr/local/Cellar/openjdk/15.0.1"
export PATH="${PATH}:/Users/leor.finkelberg/hbase/hbase-2.4.0/bin"
\end{lstlisting}

Диреткорию размещения java на MacOS X следует искать с помощью менеджера пакетов \texttt{brew}
\begin{lstlisting}[
style = bash,
numbers = none
]
brew list java # /usr/local/Cellar/openjdk/15.0.1/bin/java
\end{lstlisting}

ВАЖНО: обновить java, можно скачав соответствующую версию с ресурса \url{https://www.oracle.com/java/technologies/javase-jdk15-downloads.html}.

Запустить HBase можно с помощью сценария командной оболочки из \directory{bin/}
\begin{lstlisting}[
style = bash,
numbers = none	
]
start-hbase.sh
\end{lstlisting}

Подключиться к запущенному экземпляру можно так
\begin{lstlisting}[
style = bash,
numbers = none	
]
hbase shell
\end{lstlisting}

Для того чтобы убедиться, что процесс HMaster запущен можно воспользоваться утилитой~\texttt{jps}.

Бывает удобно следить за работой приложения с помощью Web-интерфейса, доступного на \url{http://localhost:16010}.

Закончить сессию можно с помощью команды \texttt{quit}. Затем нужно остановить HBase
\begin{lstlisting}[
style = bash,
numbers = none
]
stop-hbase.sh
\end{lstlisting}




\section{Пакетная и потоковая обработка данных}

Пакетная обработка -- обработка всего за раз без взаимодействия с конечным пользователем. Задача выполняется однократно или по расписанию, тригеру и пр.

Инструменты \emph{пакетной} обработки:
\begin{itemize}
	\item Spark -- стандарт в этой области,
	
	\item Flink -- псевдо-batch,
	
	\item Hive -- когда знаешь только SQL.
\end{itemize}

Инструменты \emph{потоковой} обработки:
\begin{itemize}
	\item Spark Streaming -- микробатчи,
	
	\item Flink -- реальный стириминг,
	
	\item Kafka Streams -- Kappa-архитектура.
\end{itemize}

Apache Kafka -- это быстрая, масштабируемая, надежная и отказоустойчивая система обмена сообщениями по механизму публикация-подписка. Еще можно сказать, что Kafka это распределенная потоковая платформа.

Если упрощенно, то Kafka предназначена для организации обмена сообщениями и результатами работы между микросервисами приложения.

Kafka работает с другими распределенными фреймверками как Spark, Samza, Flink для анализа и визуализации потоковых данных в реальном времени. Kafka хорошо интегрируется с ML фреймверками для решения ML/AI задач на потоках.

Основные определения:
\begin{itemize}
	\item Producer -- сервис, отправляющий сообщение,
	
	\item Consumer -- сервис, получающий данные,
	
	\item Broker -- один узел Kafka,
	
	\item Topic -- логическая очередь,
	
	\item Partition -- физическая часть очереди.
\end{itemize}

Обычно взаимодействие Kafka и Spark Streaming устроено следующим образом:
\begin{itemize}
	\item исходные данные записываются в топики Apache Kafka,
	
	\item приложение Spark Streaming считывает нужные данные и обрабатывает их согласно бизнес-логике,
	
	\item полученные результаты приложение Spark Streaming отправляет в место назначения -- новый топик Apache Kafka, озеро данных на базе Hadoop HDFS, аналитическую СУБД (HBase, Hive, Greenplum etc.) или BI-систему.
\end{itemize}

Топик состоит из партиций. Партиция упорядоченная и неизменяемая последовательность сообщений.

Семантика доставки:
\begin{itemize}
	\item At most once (максимум один раз) -- сообщения могут потеряны, но никогда не будут доставлены повторно (не будет дубликатов),
	
	\item At least once (минимум один раз) -- сообщения никогда не теряются, но могут быть доставлены повторно (возможны дубликаты),
	
	\item Exactly once (строго один раз) -- это то, чего на самом деле хотят люди; каждое сообщение доставляется только один раз.
\end{itemize}

Концепция и основные компоненты потоковой обработки
\begin{itemize}
	\item Structured Streaming -- передача не материализует всю таблицу сразу,
	
	\item Spark Streaming (Dstream) -- предоставляет абстракцию высокого уровня, называемую дискретным потоком или DStream, которая представляет непрерывный поток данных. DStream -- последовательность RDD.
\end{itemize}

Триггеры в Spark Streaming:
\begin{itemize}
	\item Unspecified (по умолчанию) -- если параметр триггера не указан явно, то по умолчанию запрос будет выполняться в режиме micro-batch, в котором микропакеты будут сгенерированы, как только предыдущий микропакет завершит обработку,
	
	\item Fixed interval micro-batches -- запрос будет выполнятся в режиме микропакетов, в котором микропакеты будут запускаться через указанные пользователем интервалы,
	
	\item One-time micro-batch -- запрос будет выполнять только один микропакет для обработки всех доступных данных, а затем остановится самостоятельно,
	
	\item Continuous with fixed checkpoint interval (экспериментально) -- запрос будет выполняться в новом режиме непрерывной обработки с малой задержкой.
\end{itemize}

Интеграция Kafka в Spark Streaming:
\begin{itemize}
	\item Write Ahead Logs (WAL) для Kafka -- это гарантирует, что никакие данные, полученные из любых надежных источников данных (т.е. транзакционных источников, таких как Flume, Kafka и Kinesis), не будут потеряны из-за сбоев. Даже для ненадежных (т.е. нетранзакционных) иточников, таких как простые старые сокеты, это сводит к минимуму потерю данных.
	
	\item Direct API для Kafka -- это позволяет обрабатывать каждую запись Kafka ровно один раз, несмотря на сбои, без использования журналов предварительной записи. Это делает конвейеры Spark Streaming + Kafka более эффективными, обеспечивая гарантию отказоусточивости.
\end{itemize}


\section{Приемы работы со Spark в Apache Zeppelin}

\texttt{Apache Zeppelin} \url{http://zeppelin.apache.org/download.html} -- это многофункциональная интерактивная оболочка, которая позволяет выполнять запросы к различным источникам данных, обрабатывать и визуализировать результаты, а самое главное <<из коробки>> поддерживает \texttt{Spark}. Близкий аналог \texttt{Jupyter Notebook}, но \texttt{Zeppelin} больше ориентирован на работу с базами данных. Он использует концепцию <<интерпретаторов>> -- плагинов, которые обеспечивают бекенд для какого-либо языка и/или БД.

Проще всего запустить \texttt{Zeppelin} с помощью \texttt{Docker}
\begin{lstlisting}[
style = bash,
numbers = none
]
docker run -p 8080:8080 --rm --name zeppelin apache/zeppelin:0.9.0
# или так
docker run -p 8080:8080 --rm \
  -v $(pwd)/logs:/logs \
  -v $(pwd)/notebook:/notebook \
  -e ZEPPELIN_LOG_DIR='/logs' \
  -e ZEPPELIN_NOTEBOOK_DIR='/notebook' \
  --name zeppelin apache/zeppelin:0.9.0
\end{lstlisting}

Страница \texttt{Zeppelin} будет доступна в браузере \texttt{localhost:8080}.

Подробное руководство по работе с \texttt{Apache Zeppelin} можно найти по адресу \url{https://docs.arenadata.io/aaw/Zeppelin/index.html}.




% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	\bibitem{carey:spark-2015}{ \emph{Карау Х.}, Конвински Э., Венделл П., Захария М. Изучаем Spark: молниеносный анализ данных. -- М.: ДМК Пресс, 2015. -- 304 с. }
\end{thebibliography}

%\listoffigures\addcontentsline{toc}{section}{Список иллюстраций}

\end{document}
